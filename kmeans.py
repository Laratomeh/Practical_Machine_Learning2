# -*- coding: utf-8 -*-
"""KMeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OCt-YV6s3ygimQD7kWhxYMDUEXIo0hkR

# Importing Libraries
"""

# !pip install dataprep
# !pip install -U dataprep
# !from dataprep.eda import *
# from dataprep.eda import create_report
# from dataprep.eda import plot, plot_correlation, plot_missing

import warnings
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
# !pip install contractions
# %matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
import re
# import contractions
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download('words')
from nltk.corpus import stopwords
from sklearn.feature_extraction import text
from sklearn.cluster import KMeans,MiniBatchKMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud,STOPWORDS
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.svm import SVC,LinearSVC 
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
from string import punctuation
from nltk import pos_tag
from nltk.corpus import wordnet
import string
import matplotlib.cm as cm
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

"""# Importing & Analyzing Data"""

df = pd.read_csv("/content/fake_job_postings.csv",engine='python',header=0,error_bad_lines=False)

# create_report(df)

df.head()

df.describe()

df.info()

df.shape

df.isna().sum()

unique = df.nunique()
print("Number of unique values:\n{}".format(unique))

df.groupby('fraudulent').describe()

fig1, ax1 = plt.subplots(figsize=(5,5))
colors = plt.get_cmap('Greens')(np.linspace(0.2, 0.7, len(df['fraudulent'])))
ax1.pie(df['fraudulent'].value_counts(),colors=colors, explode=(0,0.1),labels=['Not fraudulent','fraudulent'],autopct='%1.1f%%')
ax1.axis('equal')  
plt.tight_layout()
plt.legend()
plt.show()

plt.figure(figsize = (5,5))
correlation = df.corr()
sns.heatmap(correlation,cmap= "Pastel1_r")

df.head()

fraud = df[df['fraudulent']== 1]
fraud.shape

not_fraud = df[df['fraudulent']== 0]
not_fraud.shape

"""# Data Preprocessing"""

del df['telecommuting']
del df['has_company_logo']
del df['has_questions']

df.fillna(" ",inplace = True)

df.isna().sum()

df['full_text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' +df['required_experience'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function']

df.full_text[0]

df[df['full_text'].duplicated(keep=False)].sort_values('full_text').head()

# df = df.drop_duplicates('full_text')

df.shape

del df['title']
del df['location']
del df['department']
del df['company_profile']
del df['description']
del df['requirements']
del df['benefits']
del df['employment_type']
del df['required_experience']
del df['required_education']
del df['industry']
del df['function']
del df['salary_range']
del df['job_id']

plt.figure(figsize=(5, 5))
ax = sns.countplot(x='fraudulent', data=df)
ax.set_xticklabels(ax.get_xticklabels())
plt.tight_layout()

df.head()

df.shape

# Define a function to clean up the text
REPLACE_BRACKETS = re.compile('[/(){}\[\]\|@,;]')
REMOVE_NUMBERS = re.compile('[\d+]')
REMOVE_SYMBOLS = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))
words = set(nltk.corpus.words.words())

def clean(text):
    # convert text to lowercase
    text = text.lower() 

    # replace brackets by blank space
    text = REPLACE_BRACKETS.sub(' ', text) 
    
    # get rid of numbers
    text = REMOVE_NUMBERS.sub('', text)

    # get rid of symbols
    text = REMOVE_SYMBOLS.sub('', text) 
    
    # Removing contractions "abbreviations"
    text = contractions.fix(text)

    # get rid of stopwords
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
    
    # get rid of any words consist of 2 or more than 21 letters
    text = ' '.join(word for word in text.split() if (len(word) >= 3 and len(word) <= 21))

    # get rid of any word which is not in English dictionary
    text = " ".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())

    # # Stemming words
    # stemmer = PorterStemmer()
    # text = ' '.join([stemmer.stem(word) for word in text.split()])

    # Lemmatizing words
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])
    
    return text

df["full_text"] = df["full_text"].apply(clean)
df["full_text"]

"""# KMeans - Attempt 1"""

vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1,1), stop_words='english')
vectorized_text = vectorizer.fit_transform(df["full_text"])
pd.DataFrame(vectorized_text.toarray(), columns=vectorizer.get_feature_names_out()).head()

num_of_clusters = 6
score = []
for i in range(1,num_of_clusters + 1):
    kmeans = KMeans(n_clusters=i,init='k-means++',n_init=10, max_iter=600,random_state=0)
    kmeans.fit(vectorized_text)
    score.append(kmeans.inertia_)
plt.plot(range(1,num_of_clusters + 1 ),score)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.show()

pca = PCA(n_components = 2)
num_of_clusters = 6
kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.00001, random_state=0)
kmeans.fit(vectorized_text)
prediction = kmeans.predict(vectorized_text)

def top_features(tf_idf_array, prediction, num_features):
    labels = np.unique(prediction)
    top = []
    for label in labels:
        id_temp = np.where(prediction==label)
        means = np.mean(tf_idf_array[id_temp], axis = 0) 
        sorted_means = np.argsort(means)[::-1][:num_features] 
        features = vectorizer.get_feature_names()
        best_features = [(features[i], means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns = ['Top Features', 'Score'])
        top.append(df)
    return top

def plot_words(top, num_features):
    for i in range(0, len(top)):
        plt.figure(figsize=(5,5))
        plt.title(("Most Common Words in Cluster {}".format(i)), fontsize=10)
        sns.barplot(x = 'Score' , y = 'Top Features', orient = 'h' , data = top[i][:num_features])

top = top_features(vectorized_text.toarray(), prediction, 5)
plot_words(top, 5)

def top_words(data, clusters, labels, num_features):
    df = pd.DataFrame(data.todense()).groupby(clusters).mean()
    for i,r in df.iterrows():
        print('\nCluster {}:'.format(i))
        print(','.join([labels[t] for t in np.argsort(r)[-num_features:]]))
top_words(vectorized_text, prediction, vectorizer.get_feature_names_out(), 10)

# Predict the cluster of a new text
new_text = ["I'm driving through Moscow and the same traffic jams are there as before."]
new_text = pd.DataFrame(new_text, columns=["text"])
new_text = new_text["text"].apply(clean)
predicted = kmeans.predict(vectorizer.transform(new_text))
predicted

kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.00001, random_state=0).fit_predict(vectorized_text)
def plot_tsne_pca(data, labels):
    max_label = max(labels)
    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)
    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())
    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))
    index = np.random.choice(range(pca.shape[0]), size=300, replace=False)
    label_plot = labels[max_items]
    label_plot = [cm.hsv(i/max_label) for i in label_plot[index]]
    f, ax = plt.subplots(1, 2, figsize=(10, 5))
    ax[0].scatter(pca[index, 0], pca[index, 1], c=label_plot)
    ax[0].set_title('PCA Clusters Plot')
    ax[1].scatter(tsne[index, 0], tsne[index, 1], c=label_plot)
    ax[1].set_title('TSNE Clusters Plot')

plot_tsne_pca(vectorized_text, kmeans)

Y = pca.fit_transform(vectorized_text.toarray())
kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.00001, random_state=0)
fitted = kmeans.fit(Y)
plt.figure(figsize=(5, 5))
plt.scatter(Y[:, 0], Y[:, 1], c=prediction,s=40, linewidths=5)
centers = fitted.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],c='black',s=100, alpha=0.6);

"""# KMeans - Attempt 2"""

# Define a function to clean up the text
REPLACE_BRACKETS = re.compile('[/(){}\[\]\|@,;]')
REMOVE_NUMBERS = re.compile('[\d+]')
REMOVE_SYMBOLS = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))
words = set(nltk.corpus.words.words())

def clean(text):
    # convert text to lowercase
    text = text.lower() 

    # replace brackets by blank space
    text = REPLACE_BRACKETS.sub(' ', text) 
    
    # get rid of numbers
    text = REMOVE_NUMBERS.sub('', text)

    # get rid of symbols
    text = REMOVE_SYMBOLS.sub('', text) 
    
    # Removing contractions "abbreviations"
    text = contractions.fix(text)

    # get rid of stopwords
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
    
    # get rid of any words consist of 2 or more than 21 letters
    text = ' '.join(word for word in text.split() if (len(word) >= 3 and len(word) <= 21))

    # get rid of any word which is not in English dictionary
    text = " ".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())

    # Stemming words
    stemmer = PorterStemmer()
    text = ' '.join([stemmer.stem(word) for word in text.split()])

    # # Lemmatizing words
    # lemmatizer = WordNetLemmatizer()
    # text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])
    
    return text

df["full_text"] = df["full_text"].apply(clean)
df["full_text"]

vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1,1), stop_words='english')
vectorized_text = vectorizer.fit_transform(df["full_text"])
pd.DataFrame(vectorized_text.toarray(), columns=vectorizer.get_feature_names_out()).head()

num_of_clusters = 3
score = []
for i in range(1,num_of_clusters + 1):
    kmeans = KMeans(n_clusters=i,init='k-means++',n_init=10, max_iter=600,random_state=0)
    kmeans.fit(vectorized_text)
    score.append(kmeans.inertia_)
plt.plot(range(1,num_of_clusters + 1 ),score)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.show()

pca = PCA(n_components = 2)
num_of_clusters = 3
kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.00001, random_state=0)
kmeans.fit(vectorized_text)
prediction = kmeans.predict(vectorized_text)

def top_features(tf_idf_array, prediction, num_features):
    labels = np.unique(prediction)
    top = []
    for label in labels:
        id_temp = np.where(prediction==label)
        means = np.mean(tf_idf_array[id_temp], axis = 0) 
        sorted_means = np.argsort(means)[::-1][:num_features] 
        features = vectorizer.get_feature_names()
        best_features = [(features[i], means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns = ['Top Features', 'Score'])
        top.append(df)
    return top

def plot_words(top, num_features):
    for i in range(0, len(top)):
        plt.figure(figsize=(5, 5))
        plt.title(("Most Common Words in Cluster {}".format(i)), fontsize=10)
        sns.barplot(x = 'Score' , y = 'Top Features', orient = 'h' , data = top[i][:num_features])

top = top_features(vectorized_text.toarray(), prediction, 5)
plot_words(top, 5)

def top_words(data, clusters, labels, num_features):
    df = pd.DataFrame(data.todense()).groupby(clusters).mean()
    for i,r in df.iterrows():
        print('\nCluster {}:'.format(i))
        print(','.join([labels[t] for t in np.argsort(r)[-num_features:]]))
top_words(vectorized_text, prediction, vectorizer.get_feature_names_out(), 10)

# Predict the cluster of a new text
new_text = ["I'm driving through Moscow and the same traffic jams are there as before."]
new_text = pd.DataFrame(new_text, columns=["text"])
new_text = new_text["text"].apply(clean)
predicted = kmeans.predict(vectorizer.transform(new_text))
predicted

kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.00001, random_state=0).fit_predict(vectorized_text)
def plot_tsne_pca(data, labels):
    max_label = max(labels)
    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)
    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())
    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))
    index = np.random.choice(range(pca.shape[0]), size=300, replace=False)
    label_plot = labels[max_items]
    label_plot = [cm.hsv(i/max_label) for i in label_plot[index]]
    f, ax = plt.subplots(1, 2, figsize=(10, 5))
    ax[0].scatter(pca[index, 0], pca[index, 1], c=label_plot)
    ax[0].set_title('PCA Clusters Plot')
    ax[1].scatter(tsne[index, 0], tsne[index, 1], c=label_plot)
    ax[1].set_title('TSNE Clusters Plot')

plot_tsne_pca(vectorized_text, kmeans)

Y = pca.fit_transform(vectorized_text.toarray())
kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.00001, random_state=0)
fitted = kmeans.fit(Y)
plt.figure(figsize=(5, 5))
plt.scatter(Y[:, 0], Y[:, 1], c=prediction,s=40, linewidths=5)
centers = fitted.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],c='black',s=100, alpha=0.6);

"""# KMeans - Attempt 3"""

vectorizer = CountVectorizer(min_df=10, ngram_range=(1,1), stop_words='english')
vectorized_text = vectorizer.fit_transform(df["full_text"])
pd.DataFrame(vectorized_text.toarray(), columns=vectorizer.get_feature_names_out()).head()

num_of_clusters = 4
score = []
for i in range(1,num_of_clusters + 1):
    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=500,n_init=10,random_state=0)
    kmeans.fit(vectorized_text)
    score.append(kmeans.inertia_)
plt.plot(range(1,num_of_clusters + 1 ),score)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.show()

pca = PCA(n_components = 2)
num_of_clusters = 4
kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=500, tol=0.001, random_state=0)
kmeans.fit(vectorized_text)
prediction = kmeans.predict(vectorized_text)

def top_features(tf_idf_array, prediction, num_features):
    labels = np.unique(prediction)
    top = []
    for label in labels:
        id_temp = np.where(prediction==label)
        means = np.mean(tf_idf_array[id_temp], axis = 0) 
        sorted_means = np.argsort(means)[::-1][:num_features] 
        features = vectorizer.get_feature_names()
        best_features = [(features[i], means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns = ['Top Features', 'Score'])
        top.append(df)
    return top

def plot_words(top, num_features):
    for i in range(0, len(top)):
        plt.figure(figsize=(5, 5))
        plt.title(("Most Common Words in Cluster {}".format(i)), fontsize=10)
        sns.barplot(x = 'Score' , y = 'Top Features', orient = 'h' , data = top[i][:num_features])

top = top_features(vectorized_text.toarray(), prediction, 5)
plot_words(top, 5)

def top_words(data, clusters, labels, num_features):
    df = pd.DataFrame(data.todense()).groupby(clusters).mean()
    for i,r in df.iterrows():
        print('\nCluster {}:'.format(i))
        print(','.join([labels[t] for t in np.argsort(r)[-num_features:]]))
top_words(vectorized_text, prediction, vectorizer.get_feature_names_out(), 10)

# Predict the cluster of a new text
new_text = ["Sentiment Analysis means analyzing the sentiment of a given text and categorizing into a specific class or category."]
new_text = pd.DataFrame(new_text, columns=["text"])
new_text = new_text["text"].apply(clean)
predicted = kmeans.predict(vectorizer.transform(new_text))
predicted

kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=500, tol=0.001, random_state=0).fit_predict(vectorized_text)
def plot_tsne_pca(data, labels):
    max_label = max(labels)
    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)
    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())
    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))
    index = np.random.choice(range(pca.shape[0]), size=300, replace=False)
    label_plot = labels[max_items]
    label_plot = [cm.hsv(i/max_label) for i in label_plot[index]]
    f, ax = plt.subplots(1, 2, figsize=(10, 5))
    ax[0].scatter(pca[index, 0], pca[index, 1], c=label_plot)
    ax[0].set_title('PCA Clusters Plot')
    ax[1].scatter(tsne[index, 0], tsne[index, 1], c=label_plot)
    ax[1].set_title('TSNE Clusters Plot')

plot_tsne_pca(vectorized_text, kmeans)

Y = pca.fit_transform(vectorized_text.toarray())
kmeans = KMeans(n_clusters=num_of_clusters, init='k-means++', n_init=10, max_iter=500, tol=0.001, random_state=0)
fitted = kmeans.fit(Y)
plt.figure(figsize=(5, 5))
plt.scatter(Y[:, 0], Y[:, 1], c=prediction,s=40, linewidths=5)
centers = fitted.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],c='black',s=100, alpha=0.6);

"""# KMeans - Attempt 4"""

vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=50, norm='l2', ngram_range=(1,2), stop_words='english')
vectorized_text = vectorizer.fit_transform(df["full_text"])
pd.DataFrame(vectorized_text.toarray(), columns=vectorizer.get_feature_names_out()).head()

num_of_clusters = 5
score = []
for i in range(1,num_of_clusters + 1):
    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=100,n_init=50,random_state=0)
    kmeans.fit(vectorized_text)
    score.append(kmeans.inertia_)
plt.plot(range(1,num_of_clusters + 1 ),score)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.show()

pca = PCA(n_components = 2)
num_of_clusters = 2
kmeans = MiniBatchKMeans(n_clusters=num_of_clusters, init='k-means++', max_iter=100,n_init=50, tol=0.005, random_state=0)
kmeans.fit(vectorized_text)
prediction = kmeans.predict(vectorized_text)

def top_features(tf_idf_array, prediction, num_features):
    labels = np.unique(prediction)
    top = []
    for label in labels:
        id_temp = np.where(prediction==label)
        means = np.mean(tf_idf_array[id_temp], axis = 0) 
        sorted_means = np.argsort(means)[::-1][:num_features] 
        features = vectorizer.get_feature_names()
        best_features = [(features[i], means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns = ['Top Features', 'Score'])
        top.append(df)
    return top

def plot_words(top, num_features):
    for i in range(0, len(top)):
        plt.figure(figsize=(5, 5))
        plt.title(("Most Common Words in Cluster {}".format(i)), fontsize=10)
        sns.barplot(x = 'Score' , y = 'Top Features', orient = 'h' , data = top[i][:num_features])

top = top_features(vectorized_text.toarray(), prediction, 5)
plot_words(top, 5)

def top_words(data, clusters, labels, num_features):
    df = pd.DataFrame(data.todense()).groupby(clusters).mean()
    for i,r in df.iterrows():
        print('\nCluster {}:'.format(i))
        print(','.join([labels[t] for t in np.argsort(r)[-num_features:]]))
top_words(vectorized_text, prediction, vectorizer.get_feature_names_out(), 10)

# Predict the cluster of a new text
new_text = ["In computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series."]
new_text = pd.DataFrame(new_text, columns=["text"])
new_text = new_text["text"].apply(clean)
predicted = kmeans.predict(vectorizer.transform(new_text))
predicted

kmeans = MiniBatchKMeans(n_clusters=num_of_clusters, init='k-means++', max_iter=100,n_init=50, tol=0.005, random_state=0).fit_predict(vectorized_text)
def plot_tsne_pca(data, labels):
    max_label = max(labels)
    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)
    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())
    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))
    index = np.random.choice(range(pca.shape[0]), size=300, replace=False)
    label_plot = labels[max_items]
    label_plot = [cm.hsv(i/max_label) for i in label_plot[index]]
    f, ax = plt.subplots(1, 2, figsize=(10, 5))
    ax[0].scatter(pca[index, 0], pca[index, 1], c=label_plot)
    ax[0].set_title('PCA Clusters Plot')
    ax[1].scatter(tsne[index, 0], tsne[index, 1], c=label_plot)
    ax[1].set_title('TSNE Clusters Plot')

plot_tsne_pca(vectorized_text, kmeans)

Y = pca.fit_transform(vectorized_text.toarray())
kmeans = MiniBatchKMeans(n_clusters=num_of_clusters, init='k-means++', max_iter=100,n_init=50, tol=0.005, random_state=0)
fitted = kmeans.fit(Y)
plt.figure(figsize=(5, 5))
plt.scatter(Y[:, 0], Y[:, 1], c=prediction,s=40, linewidths=5)
centers = fitted.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],c='black',s=100, alpha=0.6);