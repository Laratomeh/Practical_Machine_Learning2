# -*- coding: utf-8 -*-
"""OPTICS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LpGRJ8Gghf2Z1m0nB8g_t2-45roudLqv

# Importing Libraries
"""

# !pip install dataprep
# !pip install -U dataprep
# !from dataprep.eda import *
# from dataprep.eda import create_report
# from dataprep.eda import plot, plot_correlation, plot_missing

import warnings
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
!pip install contractions
# %matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
import re
import contractions
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download('words')
from nltk.corpus import stopwords
from sklearn.feature_extraction import text
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud,STOPWORDS
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.stem import WordNetLemmatizer 
from sklearn.cluster import OPTICS, cluster_optics_dbscan
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
from scipy.optimize import linear_sum_assignment
from string import punctuation
from nltk import pos_tag
from nltk.corpus import wordnet
import string
import matplotlib.cm as cm
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from matplotlib import gridspec

"""# Importing & Analyzing Data"""

df = pd.read_csv("/content/fake_job_postings.csv",engine='python',header=0,error_bad_lines=False)

# create_report(df)

df.head()

df.describe()

df.info()

df.shape

df.isna().sum()

unique = df.nunique()
print("Number of unique values:\n{}".format(unique))

df.groupby('fraudulent').describe()

fig1, ax1 = plt.subplots(figsize=(5,5))
colors = plt.get_cmap('Greens')(np.linspace(0.2, 0.7, len(df['fraudulent'])))
ax1.pie(df['fraudulent'].value_counts(),colors=colors, explode=(0,0.1),labels=['Not fraudulent','fraudulent'],autopct='%1.1f%%')
ax1.axis('equal')  
plt.tight_layout()
plt.legend()
plt.show()

plt.figure(figsize = (5,5))
correlation = df.corr()
sns.heatmap(correlation,cmap= "Pastel1_r")

df.head()

fraud = df[df['fraudulent']== 1]
fraud.shape

not_fraud = df[df['fraudulent']== 0]
not_fraud.shape

"""# Data Preprocessing"""

del df['telecommuting']
del df['has_company_logo']
del df['has_questions']

df.fillna(" ",inplace = True)

df.isna().sum()

df['full_text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' +df['required_experience'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function']

df.full_text[0]

df[df['full_text'].duplicated(keep=False)].sort_values('full_text').head()

df.shape

plt.figure(figsize=(5, 5))
ax = sns.countplot(x='fraudulent', data=df)
ax.set_xticklabels(ax.get_xticklabels())
plt.tight_layout()

del df['title']
del df['location']
del df['department']
del df['company_profile']
del df['description']
del df['requirements']
del df['benefits']
del df['employment_type']
del df['required_experience']
del df['required_education']
del df['industry']
del df['function']
del df['salary_range']
del df['job_id']

df.head()

df.full_text

# Define a function to clean up the text
REPLACE_BRACKETS = re.compile('[/(){}\[\]\|@,;]')
REMOVE_NUMBERS = re.compile('[\d+]')
REMOVE_SYMBOLS = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))
words = set(nltk.corpus.words.words())

def clean(text):
    # convert text to lowercase
    text = text.lower() 

    # replace brackets by blank space
    text = REPLACE_BRACKETS.sub(' ', text) 
    
    # get rid of numbers
    text = REMOVE_NUMBERS.sub('', text)

    # get rid of symbols
    text = REMOVE_SYMBOLS.sub('', text) 
    
    # Removing contractions "abbreviations"
    text = contractions.fix(text)

    # get rid of stopwords
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
    
    # get rid of any words consist of 2 or more than 21 letters
    text = ' '.join(word for word in text.split() if (len(word) >= 3 and len(word) <= 21))

    # get rid of any word which is not in English dictionary
    text = " ".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())

    # # Stemming words
    # stemmer = PorterStemmer()
    # text = ' '.join([stemmer.stem(word) for word in text.split()])

    # Lemmatizing words
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])
    
    return text

df["full_text"] = df["full_text"].apply(clean)
df["full_text"]

"""# OPTICS - Attempt 1"""

Train_text , Test_text ,Train_labels , Test_labels = train_test_split(df.full_text,df.fraudulent , test_size = 0.2 , random_state = 0)

vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1,1), stop_words='english')

Train_text = vectorizer.fit_transform(df['full_text'][0:1500].astype(str))
Train_labels = df['fraudulent'][0:3000]
vectorized_train_text = vectorizer.fit_transform(df["full_text"]).todense()

Test_text = vectorizer.transform(df['full_text'][0:500].astype(str))
Test_labels = df['fraudulent'][0:500]
vectorized_test_text = vectorizer.fit_transform(df["full_text"]).todense()

model = OPTICS(metric= 'cosine', min_cluster_size = 0.25)
model.fit_predict(Train_text.toarray())
model_predicted_labels = model.labels_

labels_050 = cluster_optics_dbscan(
    reachability=model.reachability_,
    core_distances=model.core_distances_,
    ordering=model.ordering_,
    eps=0.5,)

labels_200 = cluster_optics_dbscan(
    reachability=model.reachability_,
    core_distances=model.core_distances_,
    ordering=model.ordering_,
    eps=2,)

space = np.arange(1500)
reachability = model.reachability_[model.ordering_]
labels = model.labels_[model.ordering_]

plt.figure(figsize=(10, 7))
G = gridspec.GridSpec(2, 3)
ax1 = plt.subplot(G[0, :])
ax2 = plt.subplot(G[1, 0])
ax3 = plt.subplot(G[1, 1])
ax4 = plt.subplot(G[1, 2])

# Reachability plot
colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = space[labels == klass]
    Rk = reachability[labels == klass]
    ax1.plot(Xk, Rk, color, alpha=0.3)
ax1.plot(space[labels == -1], reachability[labels == -1], "k.", alpha=0.3)
ax1.plot(space, np.full_like(space, 2.0, dtype=float), "k-", alpha=0.5)
ax1.plot(space, np.full_like(space, 0.5, dtype=float), "k-.", alpha=0.5)
ax1.set_ylabel("Reachability (epsilon distance)")
ax1.set_title("Reachability Plot")

# OPTICS
colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = Train_text.toarray()[model.labels_ == klass]
    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
ax2.plot(Train_text.toarray()[model.labels_ == -1, 0], Train_text.toarray()[model.labels_ == -1, 1], "k+", alpha=0.1)
ax2.set_title("Automatic Clustering\nOPTICS")

# DBSCAN at 0.5
colors = ["g", "greenyellow", "olive", "r", "b", "c"]
for klass, color in zip(range(0, 6), colors):
    Xk = Train_text.toarray()[labels_050 == klass]
    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3, marker=".")
ax3.plot(Train_text.toarray()[labels_050 == -1, 0], Train_text.toarray()[labels_050 == -1, 1], "k+", alpha=0.1)
ax3.set_title("Clustering at 0.5 epsilon cut\nDBSCAN")

# DBSCAN at 2.
colors = ["g.", "m.", "y.", "c."]
for klass, color in zip(range(0, 4), colors):
    Xk = Train_text.toarray()[labels_200 == klass]
    ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
ax4.plot(Train_text.toarray()[labels_200 == -1, 0], Train_text.toarray()[labels_200 == -1, 1], "k+", alpha=0.1)
ax4.set_title("Clustering at 2.0 epsilon cut\nDBSCAN")

plt.tight_layout()
plt.show()

confusion_matrix = np.zeros((2,2))
for train_label, predicted_label in zip(np.array(Train_labels, dtype=int), np.array(model_predicted_labels, dtype = int)):
    confusion_matrix[train_label][predicted_label] += 1
row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
translate = dict(zip(col_ind, row_ind))
model_predicted_labels = np.array([
    translate[label]
    for label in model_predicted_labels])
print(np.mean(model_predicted_labels == np.array(Train_labels, dtype=int)))
print(confusion_matrix)

confusion_matrix = np.zeros((2,2))
for test_label, predicted_label in zip(np.array(Test_labels, dtype=int), np.array(model_predicted_labels, dtype = int)):
    confusion_matrix[test_label][predicted_label] += 1  
row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
translate = dict(zip(col_ind, row_ind))
model_predicted_labels = np.array([
    translate[label]
    for label in model_predicted_labels])
print(np.mean(model_predicted_labels == np.array(Test_labels, dtype=int)))
print(confusion_matrix)

"""# OPTICS - Attempt 2"""

Train_text , Test_text ,Train_labels , Test_labels = train_test_split(df.full_text,df.fraudulent , test_size = 0.2 , random_state = 0)

vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1,1), stop_words='english', max_features=1000)

Train_text = vectorizer.fit_transform(df['full_text'][0:2000].astype(str))
Train_labels = df['fraudulent'][0:2000]
vectorized_train_text = vectorizer.fit_transform(df["full_text"]).todense()

Test_text = vectorizer.transform(df['full_text'][0:500].astype(str))
Test_labels = df['fraudulent'][0:500]
vectorized_test_text = vectorizer.fit_transform(df["full_text"]).todense()

model = OPTICS(cluster_method = 'dbscan',eps= 50, metric= 'cosine', min_cluster_size = 0.05)
model.fit_predict(Train_text.toarray())
model_predicted_labels = model.labels_

labels_050 = cluster_optics_dbscan(
    reachability=model.reachability_,
    core_distances=model.core_distances_,
    ordering=model.ordering_,
    eps=0.5,)

labels_200 = cluster_optics_dbscan(
    reachability=model.reachability_,
    core_distances=model.core_distances_,
    ordering=model.ordering_,
    eps=2,)

space = np.arange(2000)
reachability = model.reachability_[model.ordering_]
labels = model.labels_[model.ordering_]

plt.figure(figsize=(10, 7))
G = gridspec.GridSpec(2, 3)
ax1 = plt.subplot(G[0, :])
ax2 = plt.subplot(G[1, 0])
ax3 = plt.subplot(G[1, 1])
ax4 = plt.subplot(G[1, 2])

# Reachability plot
colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = space[labels == klass]
    Rk = reachability[labels == klass]
    ax1.plot(Xk, Rk, color, alpha=0.3)
ax1.plot(space[labels == -1], reachability[labels == -1], "k.", alpha=0.3)
ax1.plot(space, np.full_like(space, 2.0, dtype=float), "k-", alpha=0.5)
ax1.plot(space, np.full_like(space, 0.5, dtype=float), "k-.", alpha=0.5)
ax1.set_ylabel("Reachability (epsilon distance)")
ax1.set_title("Reachability Plot")

# OPTICS
colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = Train_text.toarray()[model.labels_ == klass]
    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
ax2.plot(Train_text.toarray()[model.labels_ == -1, 0], Train_text.toarray()[model.labels_ == -1, 1], "k+", alpha=0.1)
ax2.set_title("Automatic Clustering\nOPTICS")

# DBSCAN at 0.5
colors = ["g", "greenyellow", "olive", "r", "b", "c"]
for klass, color in zip(range(0, 6), colors):
    Xk = Train_text.toarray()[labels_050 == klass]
    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3, marker=".")
ax3.plot(Train_text.toarray()[labels_050 == -1, 0], Train_text.toarray()[labels_050 == -1, 1], "k+", alpha=0.1)
ax3.set_title("Clustering at 0.5 epsilon cut\nDBSCAN")

# DBSCAN at 2.
colors = ["g.", "m.", "y.", "c."]
for klass, color in zip(range(0, 4), colors):
    Xk = Train_text.toarray()[labels_200 == klass]
    ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
ax4.plot(Train_text.toarray()[labels_200 == -1, 0], Train_text.toarray()[labels_200 == -1, 1], "k+", alpha=0.1)
ax4.set_title("Clustering at 2.0 epsilon cut\nDBSCAN")

plt.tight_layout()
plt.show()

confusion_matrix = np.zeros((2,2))
for train_label, predicted_label in zip(np.array(Train_labels, dtype=int), np.array(model_predicted_labels, dtype = int)):
    confusion_matrix[train_label][predicted_label] += 1  
row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
translate = dict(zip(col_ind, row_ind))
model_predicted_labels = np.array([
    translate[label]
    for label in model_predicted_labels])
print(np.mean(model_predicted_labels == np.array(Train_labels, dtype=int)))
print(confusion_matrix)

confusion_matrix = np.zeros((2,2))
for test_label, predicted_label in zip(np.array(Test_labels, dtype=int), np.array(model_predicted_labels, dtype = int)):
    confusion_matrix[test_label][predicted_label] += 1    
row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
translate = dict(zip(col_ind, row_ind))
model_predicted_labels = np.array([
    translate[label]
    for label in model_predicted_labels])
print(np.mean(model_predicted_labels == np.array(Test_labels, dtype=int)))
print(confusion_matrix)

"""# OPTICS - Attempt 3"""

Train_text , Test_text ,Train_labels , Test_labels = train_test_split(df.full_text,df.fraudulent , test_size = 0.2 , random_state = 0)

vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, ngram_range=(1,1), stop_words='english')

Train_text = vectorizer.fit_transform(df['full_text'][0:3000].astype(str))
Train_labels = df['fraudulent'][0:3000]
vectorized_train_text = vectorizer.fit_transform(df["full_text"]).todense()

Test_text = vectorizer.transform(df['full_text'][0:800].astype(str))
Test_labels = df['fraudulent'][0:800]
vectorized_test_text = vectorizer.fit_transform(df["full_text"]).todense()

model = OPTICS(cluster_method = 'dbscan', eps= 20, metric= 'cosine', min_cluster_size = 0.25)
model.fit_predict(Train_text.toarray())
model_predicted_labels = model.labels_

labels_050 = cluster_optics_dbscan(
    reachability=model.reachability_,
    core_distances=model.core_distances_,
    ordering=model.ordering_,
    eps=0.5,)

labels_200 = cluster_optics_dbscan(
    reachability=model.reachability_,
    core_distances=model.core_distances_,
    ordering=model.ordering_,
    eps=2,)

space = np.arange(3000)
reachability = model.reachability_[model.ordering_]
labels = model.labels_[model.ordering_]

plt.figure(figsize=(10, 7))
G = gridspec.GridSpec(2, 3)
ax1 = plt.subplot(G[0, :])
ax2 = plt.subplot(G[1, 0])
ax3 = plt.subplot(G[1, 1])
ax4 = plt.subplot(G[1, 2])

# Reachability plot
colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = space[labels == klass]
    Rk = reachability[labels == klass]
    ax1.plot(Xk, Rk, color, alpha=0.3)
ax1.plot(space[labels == -1], reachability[labels == -1], "k.", alpha=0.3)
ax1.plot(space, np.full_like(space, 2.0, dtype=float), "k-", alpha=0.5)
ax1.plot(space, np.full_like(space, 0.5, dtype=float), "k-.", alpha=0.5)
ax1.set_ylabel("Reachability (epsilon distance)")
ax1.set_title("Reachability Plot")

# OPTICS
colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = Train_text.toarray()[model.labels_ == klass]
    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
ax2.plot(Train_text.toarray()[model.labels_ == -1, 0], Train_text.toarray()[model.labels_ == -1, 1], "k+", alpha=0.1)
ax2.set_title("Automatic Clustering\nOPTICS")

# DBSCAN at 0.5
colors = ["g", "greenyellow", "olive", "r", "b", "c"]
for klass, color in zip(range(0, 6), colors):
    Xk = Train_text.toarray()[labels_050 == klass]
    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3, marker=".")
ax3.plot(Train_text.toarray()[labels_050 == -1, 0], Train_text.toarray()[labels_050 == -1, 1], "k+", alpha=0.1)
ax3.set_title("Clustering at 0.5 epsilon cut\nDBSCAN")

# DBSCAN at 2.
colors = ["g.", "m.", "y.", "c."]
for klass, color in zip(range(0, 4), colors):
    Xk = Train_text.toarray()[labels_200 == klass]
    ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
ax4.plot(Train_text.toarray()[labels_200 == -1, 0], Train_text.toarray()[labels_200 == -1, 1], "k+", alpha=0.1)
ax4.set_title("Clustering at 2.0 epsilon cut\nDBSCAN")

plt.tight_layout()
plt.show()

confusion_matrix = np.zeros((2,2))
for train_label, predicted_label in zip(np.array(Train_labels, dtype=int), np.array(model_predicted_labels, dtype = int)):
    confusion_matrix[train_label][predicted_label] += 1
row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
translate = dict(zip(col_ind, row_ind))
model_predicted_labels = np.array([
    translate[label]
    for label in model_predicted_labels])
print(np.mean(model_predicted_labels == np.array(Train_labels, dtype=int)))
print(confusion_matrix)

confusion_matrix = np.zeros((2,2))
for test_label, predicted_label in zip(np.array(Test_labels, dtype=int), np.array(model_predicted_labels, dtype = int)):
    confusion_matrix[test_label][predicted_label] += 1  
row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
translate = dict(zip(col_ind, row_ind))
model_predicted_labels = np.array([
    translate[label]
    for label in model_predicted_labels])
print(np.mean(model_predicted_labels == np.array(Test_labels, dtype=int)))
print(confusion_matrix)

"""# OPTICS - Attempt 4"""

Train_text , Test_text ,Train_labels , Test_labels = train_test_split(df.full_text,df.fraudulent , test_size = 0.2 , random_state = 0)

vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1,1), stop_words='english')

Train_text = vectorizer.fit_transform(df['full_text'][0:1000].astype(str))
Train_labels = df['fraudulent'][0:1000]
vectorized_train_text = vectorizer.fit_transform(df["full_text"]).todense()

Test_text = vectorizer.transform(df['full_text'][0:300].astype(str))
Test_labels = df['fraudulent'][0:300]
vectorized_test_text = vectorizer.fit_transform(df["full_text"]).todense()

model = OPTICS(cluster_method = 'dbscan', eps= 10, metric= 'cosine', min_cluster_size = 0.25)
model.fit_predict(Train_text.toarray())
model_predicted_labels = model.labels_

labels_050 = cluster_optics_dbscan(
    reachability=model.reachability_,
    core_distances=model.core_distances_,
    ordering=model.ordering_,
    eps=0.5,)

labels_200 = cluster_optics_dbscan(
    reachability=model.reachability_,
    core_distances=model.core_distances_,
    ordering=model.ordering_,
    eps=2,)

space = np.arange(1000)
reachability = model.reachability_[model.ordering_]
labels = model.labels_[model.ordering_]

plt.figure(figsize=(10, 7))
G = gridspec.GridSpec(2, 3)
ax1 = plt.subplot(G[0, :])
ax2 = plt.subplot(G[1, 0])
ax3 = plt.subplot(G[1, 1])
ax4 = plt.subplot(G[1, 2])

# Reachability plot
colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = space[labels == klass]
    Rk = reachability[labels == klass]
    ax1.plot(Xk, Rk, color, alpha=0.3)
ax1.plot(space[labels == -1], reachability[labels == -1], "k.", alpha=0.3)
ax1.plot(space, np.full_like(space, 2.0, dtype=float), "k-", alpha=0.5)
ax1.plot(space, np.full_like(space, 0.5, dtype=float), "k-.", alpha=0.5)
ax1.set_ylabel("Reachability (epsilon distance)")
ax1.set_title("Reachability Plot")

# OPTICS
colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = Train_text.toarray()[model.labels_ == klass]
    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
ax2.plot(Train_text.toarray()[model.labels_ == -1, 0], Train_text.toarray()[model.labels_ == -1, 1], "k+", alpha=0.1)
ax2.set_title("Automatic Clustering\nOPTICS")

# DBSCAN at 0.5
colors = ["g", "greenyellow", "olive", "r", "b", "c"]
for klass, color in zip(range(0, 6), colors):
    Xk = Train_text.toarray()[labels_050 == klass]
    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3, marker=".")
ax3.plot(Train_text.toarray()[labels_050 == -1, 0], Train_text.toarray()[labels_050 == -1, 1], "k+", alpha=0.1)
ax3.set_title("Clustering at 0.5 epsilon cut\nDBSCAN")

# DBSCAN at 2.
colors = ["g.", "m.", "y.", "c."]
for klass, color in zip(range(0, 4), colors):
    Xk = Train_text.toarray()[labels_200 == klass]
    ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
ax4.plot(Train_text.toarray()[labels_200 == -1, 0], Train_text.toarray()[labels_200 == -1, 1], "k+", alpha=0.1)
ax4.set_title("Clustering at 2.0 epsilon cut\nDBSCAN")

plt.tight_layout()
plt.show()

confusion_matrix = np.zeros((2,2))
for train_label, predicted_label in zip(np.array(Train_labels, dtype=int), np.array(model_predicted_labels, dtype = int)):
    confusion_matrix[train_label][predicted_label] += 1
row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
translate = dict(zip(col_ind, row_ind))
model_predicted_labels = np.array([
    translate[label]
    for label in model_predicted_labels])
print(np.mean(model_predicted_labels == np.array(Train_labels, dtype=int)))
print(confusion_matrix)

confusion_matrix = np.zeros((2,2))
for test_label, predicted_label in zip(np.array(Test_labels, dtype=int), np.array(model_predicted_labels, dtype = int)):
    confusion_matrix[test_label][predicted_label] += 1  
row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
translate = dict(zip(col_ind, row_ind))
model_predicted_labels = np.array([
    translate[label]
    for label in model_predicted_labels])
print(np.mean(model_predicted_labels == np.array(Test_labels, dtype=int)))
print(confusion_matrix)